{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing & Word embeddings\n",
    "\n",
    "## Introduction to word embeddings\n",
    "\n",
    "* word representation\n",
    "    * dictionary & one-hot vector\n",
    "        * downstream algorithm cannot learn from word-similarity (apple/orange juice)\n",
    "            * product of two one-hot vectors from a dictionary is zero\n",
    "    * feature representation\n",
    "        * embeddings (embedding a word into high-dimensional space)\n",
    "        * visualization in 2D space through t-SNE\n",
    "* application\n",
    "    * leveraging unlabeled text (download from internet) in downstream tasks through embeddings\n",
    "    * transfer learning\n",
    "        * learn word embeddings from large corpus (or get pre-trained embeddings)\n",
    "        * transfer embedding to new task with smaller training set\n",
    "        * optional -> continue to fine-tune the embeddings with new data\n",
    "* word embeddings captures analogies (man->women, king->queen)c,t\n",
    "    * $e_{man} - e_{woman} \\sim e_{king}-e_?$, argmax similarity $sim(e_w,e_{king} - e_{man} + e_{woman} )$\n",
    "    * helps intuition about how the embeddings internally works\n",
    "    * similarities\n",
    "        * cosine $sim(u,v) = \\frac{u^Tv}{\\|u\\|_2 \\|v\\|_2}$\n",
    "            * if angle zero then large, otherwise small\n",
    "            * works good for analogy reasoning tasks\n",
    "        * squared distance $sim(u,v) = - \\|u-v\\|^2$\n",
    "            * works ok but not that often used\n",
    "    * matrix representation E (rows embedding dim, columns word dim)\n",
    "        * $E$ (100, 10k) $\\cdot O_{6257}$ (10k, 1) = $\\begin{bmatrix} \\end{bmatrix}$ (300,1), that is embedding for the particular word 6257 from the dict, vertical slice from the $E$ matrix\n",
    "\n",
    "**Learning embedding representation**\n",
    "* neural language model\n",
    "    * [paper](https://papers.nips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)\n",
    "    * $o$ -> $E$ -> $e$ -> FC (W,b) -> softmax (W,b) -> predicting the next word in the sentence\n",
    "    * fixed window for how many words needed for the prediction\n",
    "* other context/target pairs\n",
    "    * prediction of a missing word using left and right window\n",
    "    * prediction of a next word by a word\n",
    "    * nearby 1 word (skip-grams)\n",
    "* word2vec\n",
    "    * [paper](https://arxiv.org/pdf/1301.3781)\n",
    "    * context-target pairs\n",
    "        * randomly pick a context word, randomly pick a target within a window (+-5)\n",
    "    * learning mapping from context C (\"context\") -> target T (\"juice\")\n",
    "    * $O_c$ -> $E$ -> $e_c = E\\cdot o_c$ -> softmax layer -> $\\hat{y}$\n",
    "        * softmax $p(t|c) = \\frac{e^{\\Theta_t^T e_c}}{\\sum_{j=1}^{10000} e^{\\Theta_j^T e_c}}$\n",
    "        * vocab size 10000, $\\Theta_t$ -> parameters associated with output t\n",
    "    * $L(\\hat{y},y) = - \\sum_{i=1}^{10000} y_i log \\hat{y_i}$\n",
    "    * softmax computationally intensive\n",
    "        * hierarchical softmax, tree of classifiers, unbalanced tree, computational cost $log |v|$\n",
    "    * sampling smartly (ie reducing probability of sampling a, the, or, and, ...)\n",
    "* negative sampling\n",
    "    * [paper](https://arxiv.org/pdf/1310.4546)\n",
    "    * new learning problem -> presenting context-target pairs and predicting if they are present or not (positive or negative example)\n",
    "    * we pick a context-target word and create positive example, for k examples we pick context-word and generate random words from the dictionary\n",
    "    * supervised problem -> do those terms appear together?\n",
    "    * for small datasets k = 5-20, for large ones k = 2-5\n",
    "    * binary classification $P(y=1| c,t) = \\sigma (\\Theta^T_t e_c)$\n",
    "    * 10000 binary classification problems, k+1 classifiers (random sampled + target)\n",
    "    * sampling between the observed frequency and uniform distribution $P(w_i)=\\frac{f(w_i)^{3/4}}{\\sum_{j=1}^{10000}f(w_j)^{3/4}}$ \n",
    "\n",
    "* glove (global vectors for word representation)\n",
    "    * [paper](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "    * $X_{i,j} = $ # times $j$ appears in context of $i$, might not be symmetrical if windows are too asymetrical, analogical to $X_{c,t}$\n",
    "    * $min \\sum_{i=1}^{10000} \\sum_{j=1}^{10000} f(X_{i,j}) (\\Theta_i^{T} e_j + b_i + b_j' -log X_{i,j})$\n",
    "        * searching for $\\Theta_i^{T}$\n",
    "        * weighting term $f(X_{i,j})$ to solve for a case when $X_{i,j}=0$ (with practical convention 0 log 0 = 0)\n",
    "            * can reduce weight of frequent stop words, increase weight of infrequent words\n",
    "            * $e_w^{final} = \\frac{e_w + \\Theta_w}{2}$, as $\\Theta$ and $e$ are symmetric\n",
    "* featurization of embeddings -> not interpretable axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "* sentiment classification, text to ranking, class\n",
    "    * usually small datasets, leveraging embeddings help\n",
    "    * sum/average embeddings and pass result to softmax/different classifier (ignores order)\n",
    "    * embeddings feeded to many-to-one rnn (levereges order)\n",
    "* bias in embeddings\n",
    "    * [paper](https://arxiv.org/pdf/1607.06520)\n",
    "    * man is to computer programmer as woman to homemaker\n",
    "    * embeddings can reflect bias in the texts\n",
    "    * rectification\n",
    "       \n",
    "        * identify bias direction\n",
    "            * take diffs between gendered words and average them to get the gender direction in the space\n",
    "            * project bias direction (x) vs other directions (y)\n",
    "        * neutralize\n",
    "            * for every word that is not definitional, project to get rid of bias\n",
    "        * equalize pairs\n",
    "            * ie grandmother vs grandfather are equidistant from neutral words such as babysitter or doctor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
