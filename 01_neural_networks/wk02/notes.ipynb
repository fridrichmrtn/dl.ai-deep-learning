{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression as a neural network\n",
    "\n",
    "**Notation**\n",
    "\n",
    "* assigning predicted category based on observed feature values\n",
    "* an observation $ (x,y); x \\in R; y\\in \\{0,1\\}$\n",
    "* $m$ training examples $\\{(x^{(1)},y^{(1)}), (x^{(1)},y^{(1)}), ..., (x^{(m)},y^{(m)})\\}$\n",
    "    * $M_{train}; M_{test}$\n",
    "\n",
    "* X\n",
    "    * conveniently $X = \\begin{bmatrix} \\\\ x^{(1)}  x^{(2)} ... x^{(m)} \\\\\\\\\\end{bmatrix}$, as opposed to the traditional row-wise convention (makes implementation more straightforward)\n",
    "    * matrix $X \\in R^{N_x \\cdot m}$ \n",
    "* Y\n",
    "    * conveniently $Y = \\begin{bmatrix} y^{(1)} y^{(2)} ... y^{(m)}\\end{bmatrix}$\n",
    "    * matrix $Y \\in R^{1 \\cdot m}$\n",
    "\n",
    "**Logistic regression**  \n",
    "\n",
    "* the goal of binary classification is $P(y=1|X)$, using parameters $w \\in R^{N_x}$ and $b \\in R$\n",
    "* output $\\hat{y} = \\sigma(w^T \\cdot x+b)$\n",
    "* $\\sigma (z) = \\frac{1}{1+e^{-z}}$\n",
    "    * if $z$ large $\\sigma(z) \\sim \\frac{1}{1+0} \\sim 1$\n",
    "    * if $z$ small $\\sigma(z) \\sim \\frac{1}{1+large} \\sim 0$ \n",
    "\n",
    "**Cost function**\n",
    "\n",
    "Loss function\n",
    "* single example  \n",
    "* $L(\\hat{y},y) = -(y \\log{\\hat{y}}+(1-y)\\log(1-\\hat{y}))$\n",
    "    * if $y=1$: $L(\\hat{y},y) = -\\log(\\hat{y})$ <- we want $\\hat{y}$ to be large\n",
    "    * if $y=0$: $L(\\hat{y},y) = -(1-\\log(\\hat{y}))$ <- we want $\\hat{y}$ small\n",
    "\n",
    "Cost function\n",
    "* average loss function for all observations\n",
    "* $J(w,b) = \\frac{1}{m}\\sum_{i=1}^m L(\\hat{y}^{(i)},y^{(i)}) = -\\frac{1}{m}\\sum_{i=1}^{m} y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)})\\log(1-\\hat{y}^{(i)})$\n",
    "\n",
    "**Gradient descent**\n",
    "\n",
    "* algorithm used for iteratively minimizing $J(w, b)$, useful for convex optimization problems\n",
    "* update steps    \n",
    "    * $w:= w - \\alpha\\frac{\\delta J(w,b)}{\\delta w}$\n",
    "    * $b:= b- \\alpha\\frac{\\delta J(w,b)}{\\delta b}$\n",
    "    * $\\alpha$ represents learning rate (speed of weight updates)\n",
    "\n",
    "**Derivatives**\n",
    "\n",
    "* slope of a function, formally $\\lim_{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}$\n",
    "* might differ on different point of a function \n",
    "\n",
    "**Computation graph**\n",
    "\n",
    "* based on the chain rule -> $\\frac{dJ}{da} = \\frac{dJ}{dv} \\frac{dv}{da}$\n",
    "* allows to reuse (cache) derivatives on the computational graph\n",
    "\n",
    "**Logistic regression derivatives**\n",
    "\n",
    "* use chain rule to get from the loss to the weight and bias changes\n",
    "* $\\frac{\\delta L(\\hat{y},y)}{\\delta \\hat{y}} \\frac{\\delta \\hat{y}}{\\delta z} \\frac{\\delta z}{\\delta w}$, where the first two partial derivatives can be precomputed, similarly for bias term\n",
    "\n",
    "Step-by-step implementation  \n",
    "* $J = 0; dw_1 = 0; dw_2 = 0; db = 0$\n",
    "* for i=1 to m (number of examples):\n",
    "    * $z^{(i)}=w^Tx^{(i)}+b$\n",
    "    * $a^{(i)}=\\sigma(z^{(i)})$\n",
    "    * updates\n",
    "        * $J += -\\frac{1}{m}[y\\log(a) + (1-y)\\log(1-a)]$\n",
    "        * $dw_1 += \\frac{1}{m} dz^{(i)} x_1^{(i)}$\n",
    "        * $dw_2 += \\frac{1}{m} dz^{(i)} x_2^{(i)}$\n",
    "        * $db += \\frac{1}{m} dz^{(i)}$\n",
    "\n",
    "* final update\n",
    "    * $w_1 = w_1-\\alpha dw_1$\n",
    "    * $w_2 = w_2-\\alpha dw_2$\n",
    "    * $b = b-\\alpha db$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and vectorization\n",
    "\n",
    "**Vectorization**\n",
    "\n",
    "* ability to execute loop-based operation without explicitly looping through elements\n",
    "* SIMD instructions for parallelization on CPU/GPU (ie `np.dot`)\n",
    "* avoid explicit for-loop whenever possible\n",
    "* `numpy` built-in functions support vectrization for scalar multiplication, matrix multiplication, exp/log operations, and others\n",
    "\n",
    "**Broadcasting**\n",
    "\n",
    "* do aggregate operation on parts of the whole matrix using correct `axis`, then broadcast the element-wise operation to desired dimension (with possible use of `reshape` or `keepdims`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression cost function\n",
    "\n",
    "* $p(y|x) = \\hat{y}^y \\cdot (1-\\hat{y})^{(1-y)}$\n",
    "* we can leverage log as the max of the log func is the same as in the original one, that is after simplification $\\log(p(y|x)) = y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})$\n",
    "* reasoning\n",
    "    * $ p(...) = \\Pi_{i=1}^m p(y^{(i)}|x^{(i)})$\n",
    "    * $ log(p(...)) = \\sum_{i=1}^m log(p(y^{(i)}|x^{(i)})) = - \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})$\n",
    "    * cost $J(w,b) = \\frac{1}{m}sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
