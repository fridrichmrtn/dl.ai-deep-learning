{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case studies\n",
    "\n",
    "## Classic networks\n",
    "\n",
    "**LeNet5**\n",
    "* [paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n",
    "* goal is to recognize hand-written digit from gray-scale image\n",
    "* building blocks\n",
    "    * input layer (32 x 32 x 1)\n",
    "    * CNN1\n",
    "        * 6 filters with size (5 x 5), stride 1 and no padding\n",
    "        * output (28 x 28 x 6)\n",
    "        * average-pooling, filter (2 x 2), stride 2\n",
    "        * output (14 x 14 x 6)\n",
    "    * CNN2\n",
    "        * 16 filters with size (5 x 5), stride 1 and no padding\n",
    "        * output (10 x 10 x 16)\n",
    "        * average-pooling, filter (2 x 2), stride 2\n",
    "        * output (5 x 5 x 16)\n",
    "        * flatten to 400 units\n",
    "    * FC1\n",
    "        * 120 units\n",
    "    * FC2\n",
    "        * 84 units\n",
    "    * output layer\n",
    "        * 10 units\n",
    "        * softmax activation\n",
    "        * original implementation used different approach -> euclidean radial basis function\n",
    "\n",
    "Summary  \n",
    "* 60k params\n",
    "* with depth, $n_h$, $n_w$ decreases, $n_c$ increases\n",
    "* conv + pool, conv + pool, fc & fc & output\n",
    "* sigmoid & tanh no ReLU\n",
    "\n",
    "**AlexNet**\n",
    "* [paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "* building blocks\n",
    "    * input layer (227 x 227 x 3)\n",
    "    * CNN1\n",
    "        * 96 filters with size (11 x 11), stride 4\n",
    "        * output (55 x 55 x 96)\n",
    "        * max-pooling, filter (3 x 3), stride 2\n",
    "        * output (27 x 27 x 96)\n",
    "    * CNN2\n",
    "        * 256 filters with size (5 x 5), same padding\n",
    "        * output (27 x 27 x 256)\n",
    "        * max-pooling, filter (3 x 3), stride 2\n",
    "        * output (13 x 13 x 256)\n",
    "    * CNN3-5\n",
    "        * 384 filters with size (3 x 3), same padding\n",
    "        * output (13 x 13 x 384)\n",
    "        * 384 filters with size (3 x 3), same padding\n",
    "        * output (13 x 13 x 384)\n",
    "        * 256 filters with size (3 x 3), same padding\n",
    "        * output (13 x 13 x 256)\n",
    "        * max-pooling, filter (3 x 3), stride 2\n",
    "        * output (6 x 6 x 256)\n",
    "        * flatten to 9216 units\n",
    "    * FC1\n",
    "        * 4096 units\n",
    "    * FC2\n",
    "        * 4096 units\n",
    "    * output layer\n",
    "        * 1000 units\n",
    "        * softmax activation\n",
    "\n",
    "Summary  \n",
    "* 60m params\n",
    "* large ImageNet dataset\n",
    "* ReLU activations\n",
    "* distributed training on multiple GPU\n",
    "* local response normalization layer (normalization across filter position), this does not seem to help much\n",
    "\n",
    "**VGG16**\n",
    "* [paper](https://arxiv.org/pdf/1409.1556)\n",
    "* 16 layers of params\n",
    "* building blocks\n",
    "    * input layer (224 x 224 x 3)\n",
    "    * CNN1-2\n",
    "        * 64 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (224 x 224 x 3)\n",
    "        * 64 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (224 x 224 x 64)\n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (112 x 112 x 64)\n",
    "    * CNN3-4\n",
    "        * 128 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (112 x 112 x 128)\n",
    "        * 128 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (112 x 112 x 128)\n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (56 x 56 x 128)\n",
    "    * CNN5-7\n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (56 x 56 x 256)       \n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (56 x 56 x 256)     \n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (56 x 56 x 256)\n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (28 x 28 x 256)\n",
    "    * CNN8-11\n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (28 x 28 x 512)              \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (28 x 28 x 512)            \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (28 x 28 x 512)    \n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (14 x 14 x 512)\n",
    "    * CNN12-14\n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (14 x 14 x 512)              \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (14 x 14 x 512)            \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (14 x 14 x 512)    \n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (7 x 7 x 512)\n",
    "        * flatten to 25088 units\n",
    "    * FC1-2\n",
    "        * 4096 units\n",
    "        * ReLU activation\n",
    "        * 4096 units\n",
    "        * ReLU activation\n",
    "    * output layer\n",
    "        * 1000 units\n",
    "        * softmax activation\n",
    "\n",
    "Summary  \n",
    "* 138m params\n",
    "* uniform (symmetrical?) architecture\n",
    "* with depth, $n_h$, $n_w$ decreases by factor of 2, $n_c$ increases by factor of 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ResNet\n",
    "\n",
    "* [paper](https://arxiv.org/pdf/1512.03385)\n",
    "* vanishing & exploding gradients addressed through skip connections\n",
    "* residual block\n",
    "    * activations in consecutive layers $a^{[l]}$, $a^{[l+1]}$, $a^{[l+2]}$,...\n",
    "    * main path\n",
    "        * $z^{[l+1]} = W^{[l+1]}\\cdot a^{[l]} + b^{[l+1]}$\n",
    "        * $a^{[l+1]} = g(z^{[l+1]})$, apply ReLU\n",
    "        * $z^{[l+2]} = W^{[l+2]}\\cdot a^{[l+1]} + b^{[l+2]}$\n",
    "        * $a^{[l+2]} = g(z^{[l+2]})$, apply ReLU    \n",
    "    * shortcut path (skip connection)\n",
    "        * $a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$, apply ReLU\n",
    "        * feeding signal from the initial activation into the deeper activations\n",
    "    * allows for training very deep networks\n",
    "* stacking residual blocks on each other\n",
    "* plain nets - in practice more layers lead to error increase\n",
    "* res-nets - deeper networks lead to error decrease\n",
    "\n",
    "\n",
    "* building blocks\n",
    "    * input layer\n",
    "    * CNN1\n",
    "        * 64 filters with size (7 x 7), stride 2, same padding?\n",
    "        * max-pool layer, stride 2\n",
    "    * CNN2-7\n",
    "        * first layer incl pooling with stride 2, first skip connection scaled with $W_s$\n",
    "        * 64 filters with size (3 x 3), stride 1, same padding\n",
    "        * skip connections two layers apart\n",
    "    * CNN8-16\n",
    "        * first layer incl pooling with stride 2, first skip connection scaled with $W_s$ \n",
    "        * 128 filters with size (3 x 3), stride 1, same padding\n",
    "        * skip connections two layers apart\n",
    "    * CNN17-29\n",
    "        * first layer incl pooling with stride 2, first skip connection scaled with $W_s$ \n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * skip connections two layers apart\n",
    "    * CNN30-36\n",
    "        * first layer incl pooling with stride 2, first skip connection scaled with $W_s$   \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * skip connections two layers apart\n",
    "        * ending with average pooling layer   \n",
    "    * output layer\n",
    "        * 1000 units\n",
    "        * softmax activation\n",
    "\n",
    "**Intuition example** \n",
    "* $x$ -> |big NN| -> $a^{[l]}$ (plain big NN)\n",
    "* $x$ -> |big NN| -> $a^{[l]}$ -> FC1 -> FC2 -> $a^{[l+2]}$ (res-NN)\n",
    "    * skip connection between $a^{[l]}$ & $a^{[l+2]}$\n",
    "    * ReLU activations, this $a \\geq 0$\n",
    "    * $a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$ (skip connection)\n",
    "    * $g(z^{[l+2]}+a^{[l]}) = g(W^{[l+2]}\\cdot a^{[l+1]} + b^{[l+2]} + a^{[l]})$\n",
    "        * if weight decay is applied it might lead to making $W^{[l+2]}$ and $b^{[l+2]}$ close to zero\n",
    "        * thus $a^{[l+2]} = g(a^{[l]}) = a^{[l]}$\n",
    "        * identity function is easy to learn with residual block!\n",
    "    * for the core operation $a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$ to work, $z^{[l+2]}$ and $a^{[l]}$ must have same dimensions, thus same padding is used in CNN layers, or scaling matrix is added to modify the operation to $a^{[l+2]} = g(z^{[l+2]}+W_{s}\\cdot a^{[l]})$, where scaling matrix $W_{s}$ can be learned or fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception\n",
    "\n",
    "**Inception building block - network in network**\n",
    "* [paper](https://arxiv.org/pdf/1312.4400)\n",
    "* (1 x 1) convolution\n",
    "* for 1 channel and same filter sizes, it just scales the original channel\n",
    "* for multiple channel input (6 x 6 x 32) with filter (1 x 1 x 32) and activation, output would be (6 x 6 x 32)\n",
    "    * this be perceived as full-connection across channels with activation applied (this is why network in network)\n",
    "* useful for shrinking number of filters (as opposed to filter size reduced by pooling)\n",
    "* for other cases (same number of channels), it just adds non-linearity and allows for more complex patterns\n",
    "\n",
    "**Motivation**\n",
    "* [paper](https://arxiv.org/pdf/1409.4842)\n",
    "* decisions about NN architecture are complicated, why not to use every operation at the same time\n",
    "\n",
    "**Intuition example**\n",
    "* inception block\n",
    "    * input (28 x 28 x 192)\n",
    "    * stacking multiple blocks together\n",
    "        * 64 filters with size (1 x 1), stride 1, output (28 x 28 x 64)\n",
    "        * 128 filters with size (3 x 3), stride 1, same padding, output (28 x 28 x 128)\n",
    "        * 32 filters with size (5 x 5), stride 1, same padding, output (28 x 28 x 32)\n",
    "        * max-pool layer, stride 1, same padding, output (28 x 28 x 32)\n",
    "    * output (28 x 28 x 256)\n",
    "\n",
    "* computational cost example\n",
    "    * input (28 x 28 x 192), 32 filters with size (5 x 5), stride 1, same padding, output (28 x 28 x 32)\n",
    "    * 32 filters of size 5 * 5 * 192, with 28 * 28 * 32 multiplications, thus ~120m operations\n",
    "    \n",
    "* optimized example\n",
    "    * input (28 x 28 x 192)\n",
    "    * 16 filters with size (1 x 1), output (28 x 28 x 16)\n",
    "    * 32 filters with size (5 x 5), stride 1, same padding, output (28 x 28 x 32)\n",
    "    * computational costs 28 * 28 * 16 * 192 ~ 2.4m, 28 * 28 * 32 * 5 * 5 *16 ~ 10m, thus ~12.4m operations\n",
    "\n",
    "**Building blocks**\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
