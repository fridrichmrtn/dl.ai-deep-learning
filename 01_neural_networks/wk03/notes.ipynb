{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow neural network\n",
    "\n",
    "**Notation**\n",
    "\n",
    "* superscript w/ square brackets refers to NN layer, superscript w/ round brackets refers to observation\n",
    "* input (features) /hidden (not-observed in the data)/output (target) layers\n",
    "* example of a NN with one input, one hidden and one output layer  \n",
    "\n",
    "    * input layer $x = a^{[0]}$\n",
    "    * hidden layer with 4 elements $a^{[1]} = \\begin{bmatrix} a_1^{[1]} \\\\ ... \\\\ a_4^{[1]} \\end{bmatrix}$\n",
    "    * output layer $\\hat{y} = a^{[2]}$\n",
    "\n",
    "* hidden and output layers store internal parameters $w$ and $b$\n",
    "\n",
    "**Forward pass**\n",
    "\n",
    "Hidden layer  \n",
    "\n",
    "* vectorization of the operations, ie $W^{[1]} \\cdot X + b^{[1]} = Z^{[1]}$\n",
    "* to see the inner elements of the pass\n",
    "\n",
    "    * $Z^{[1]} = \\begin{bmatrix} w_1^{[1] T} \\\\ w_2^{[1] T} \\\\ w_3^{[1] T}\\\\ w_4^{[1] T}\\\\\\end{bmatrix} \\cdot \n",
    "    \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} +\n",
    "    \\begin{bmatrix} b_1^{[1]} \\\\ b_2^{[1]} \\\\ b_3^{[1]}\\\\ b_4^{[1]}\\\\\\end{bmatrix} =\n",
    "    \\begin{bmatrix} z_1^{[1]} \\\\ z_2^{[1]} \\\\ z_3^{[1]}\\\\ z_4^{[1]}\\\\\\end{bmatrix} $\n",
    "    * activation applied element-wise on  $Z$ $ a^{[1]} = \\begin{bmatrix} a_1^{[1]} \\\\ ... \\\\ a_2^{[1]} \\end{bmatrix} = \\sigma(Z^{[1]} )$\n",
    "\n",
    "* analogous for other hidden layer\n",
    "* output layer same in log res\n",
    "\n",
    "**Vectorizing across multiple examples**\n",
    "\n",
    "* for a simple NN classifier, with one hidden and one output layer\n",
    "    * $Z^{[1]} = W^{[1]} \\cdot X + b^{[1]}$\n",
    "    * $A^{[1]} = \\sigma (Z^{[1]} )$\n",
    "    * $Z^{[2]} = W^{[2]} \\cdot A^{[1]}  + b^{[2]}$\n",
    "    * $ A^{[2]} = \\sigma (Z^{[2]} ) = \\hat{y} $\n",
    "    * all matrices on LHS have elements vertically stacked (examples), horizontal dimension contains units (neurons)\n",
    "\n",
    "**Activation functions**\n",
    "\n",
    "* sigmoid $a = \\frac{1}{1+e^{-z}}$ (almost never used except for output)\n",
    "* tangent $a = \\frac{e^z-e^z}{e^z+e^{-z}}$ (scaled version of sigmoid, centers data in hidden layers improving the learning speed)\n",
    "* relu $a = argmax(0, x)$ (improves learning as derivatives are large)\n",
    "* leaky relu $a = argmax(0.01z, x)$ (improves learning as derivatives are large)\n",
    "* non-linear activation functions make sure that more complex systems then just linear ones can be learned\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "* $\\frac{\\delta L(a,y)}{\\delta a} \\frac{\\delta a}{\\delta z} \\frac{\\delta z}{\\delta w} \\frac{\\delta w}{\\delta a} (output) \\frac{\\delta a}{\\delta z} \\frac{\\delta z}{\\delta w} \\frac{\\delta w}{\\delta x} (hidden)$\n",
    "* make sure that matrices have correct dimensions\n",
    "\n",
    "**Random initialization**\n",
    "\n",
    "* cannot initialize weights at 0, as they will be not able to learn different things (they will be symmetrical)\n",
    "* small random values improves convergence for sigmoid/tanh (small gradients if numbers large)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
