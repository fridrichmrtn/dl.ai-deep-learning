{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case studies\n",
    "\n",
    "## Classic networks\n",
    "\n",
    "**LeNet5**\n",
    "* [paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n",
    "* goal is to recognize hand-written digit from gray-scale image\n",
    "* architecture\n",
    "    * input layer (32 x 32 x 1)\n",
    "    * CNN1\n",
    "        * 6 filters with size (5 x 5), stride 1 and no padding\n",
    "        * output (28 x 28 x 6)\n",
    "        * average-pooling, filter (2 x 2), stride 2\n",
    "        * output (14 x 14 x 6)\n",
    "    * CNN2\n",
    "        * 16 filters with size (5 x 5), stride 1 and no padding\n",
    "        * output (10 x 10 x 16)\n",
    "        * average-pooling, filter (2 x 2), stride 2\n",
    "        * output (5 x 5 x 16)\n",
    "        * flatten to 400 units\n",
    "    * FC1\n",
    "        * 120 units\n",
    "    * FC2\n",
    "        * 84 units\n",
    "    * output layer\n",
    "        * 10 units\n",
    "        * softmax activation\n",
    "        * original implementation used different approach -> euclidean radial basis function\n",
    "\n",
    "Summary  \n",
    "* 60k params\n",
    "* with depth, $n_h$, $n_w$ decreases, $n_c$ increases\n",
    "* conv + pool, conv + pool, fc & fc & output\n",
    "* sigmoid & tanh no ReLU\n",
    "\n",
    "**AlexNet**\n",
    "* [paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "* architecture\n",
    "    * input layer (227 x 227 x 3)\n",
    "    * CNN1\n",
    "        * 96 filters with size (11 x 11), stride 4\n",
    "        * output (55 x 55 x 96)\n",
    "        * max-pooling, filter (3 x 3), stride 2\n",
    "        * output (27 x 27 x 96)\n",
    "    * CNN2\n",
    "        * 256 filters with size (5 x 5), same padding\n",
    "        * output (27 x 27 x 256)\n",
    "        * max-pooling, filter (3 x 3), stride 2\n",
    "        * output (13 x 13 x 256)\n",
    "    * CNN3-5\n",
    "        * 384 filters with size (3 x 3), same padding\n",
    "        * output (13 x 13 x 384)\n",
    "        * 384 filters with size (3 x 3), same padding\n",
    "        * output (13 x 13 x 384)\n",
    "        * 256 filters with size (3 x 3), same padding\n",
    "        * output (13 x 13 x 256)\n",
    "        * max-pooling, filter (3 x 3), stride 2\n",
    "        * output (6 x 6 x 256)\n",
    "        * flatten to 9216 units\n",
    "    * FC1\n",
    "        * 4096 units\n",
    "    * FC2\n",
    "        * 4096 units\n",
    "    * output layer\n",
    "        * 1000 units\n",
    "        * softmax activation\n",
    "\n",
    "Summary  \n",
    "* 60m params\n",
    "* large ImageNet dataset\n",
    "* ReLU activations\n",
    "* distributed training on multiple GPU\n",
    "* local response normalization layer (normalization across filter position), this does not seem to help much\n",
    "\n",
    "**VGG16**\n",
    "* [paper](https://arxiv.org/pdf/1409.1556)\n",
    "* 16 layers of params\n",
    "* architecture\n",
    "    * input layer (224 x 224 x 3)\n",
    "    * CNN1-2\n",
    "        * 64 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (224 x 224 x 3)\n",
    "        * 64 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (224 x 224 x 64)\n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (112 x 112 x 64)\n",
    "    * CNN3-4\n",
    "        * 128 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (112 x 112 x 128)\n",
    "        * 128 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (112 x 112 x 128)\n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (56 x 56 x 128)\n",
    "    * CNN5-7\n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (56 x 56 x 256)       \n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (56 x 56 x 256)     \n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (56 x 56 x 256)\n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (28 x 28 x 256)\n",
    "    * CNN8-11\n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (28 x 28 x 512)              \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (28 x 28 x 512)            \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (28 x 28 x 512)    \n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (14 x 14 x 512)\n",
    "    * CNN12-14\n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (14 x 14 x 512)              \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (14 x 14 x 512)            \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (14 x 14 x 512)    \n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (7 x 7 x 512)\n",
    "        * flatten to 25088 units\n",
    "    * FC1-2\n",
    "        * 4096 units\n",
    "        * ReLU activation\n",
    "        * 4096 units\n",
    "        * ReLU activation\n",
    "    * output layer\n",
    "        * 1000 units\n",
    "        * softmax activation\n",
    "\n",
    "Summary  \n",
    "* 138m params\n",
    "* uniform (symmetrical?) architecture\n",
    "* with depth, $n_h$, $n_w$ decreases by factor of 2, $n_c$ increases by factor of 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ResNet\n",
    "\n",
    "* [paper](https://arxiv.org/pdf/1512.03385)\n",
    "* vanishing & exploding gradients addressed through skip connections\n",
    "* residual block\n",
    "    * activations in consecutive layers $a^{[l]}$, $a^{[l+1]}$, $a^{[l+2]}$,...\n",
    "    * main path\n",
    "        * $z^{[l+1]} = W^{[l+1]}\\cdot a^{[l]} + b^{[l+1]}$\n",
    "        * $a^{[l+1]} = g(z^{[l+1]})$, apply ReLU\n",
    "        * $z^{[l+2]} = W^{[l+2]}\\cdot a^{[l+1]} + b^{[l+2]}$\n",
    "        * $a^{[l+2]} = g(z^{[l+2]})$, apply ReLU    \n",
    "    * shortcut path (skip connection)\n",
    "        * $a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$, apply ReLU\n",
    "        * feeding signal from the initial activation into the deeper activations\n",
    "    * allows for training very deep networks\n",
    "* stacking residual blocks on each other\n",
    "* plain nets - in practice more layers lead to error increase\n",
    "* res-nets - deeper networks lead to error decrease\n",
    "\n",
    "\n",
    "* architecture\n",
    "    * input layer\n",
    "    * CNN1\n",
    "        * 64 filters with size (7 x 7), stride 2, same padding?\n",
    "        * max-pool layer, stride 2\n",
    "    * CNN2-7\n",
    "        * first layer incl pooling with stride 2, first skip connection scaled with $W_s$\n",
    "        * 64 filters with size (3 x 3), stride 1, same padding\n",
    "        * skip connections two layers apart\n",
    "    * CNN8-16\n",
    "        * first layer incl pooling with stride 2, first skip connection scaled with $W_s$ \n",
    "        * 128 filters with size (3 x 3), stride 1, same padding\n",
    "        * skip connections two layers apart\n",
    "    * CNN17-29\n",
    "        * first layer incl pooling with stride 2, first skip connection scaled with $W_s$ \n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * skip connections two layers apart\n",
    "    * CNN30-36\n",
    "        * first layer incl pooling with stride 2, first skip connection scaled with $W_s$   \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * skip connections two layers apart\n",
    "        * ending with average pooling layer   \n",
    "    * output layer\n",
    "        * 1000 units\n",
    "        * softmax activation\n",
    "\n",
    "**Intuition example** \n",
    "* $x$ -> |big NN| -> $a^{[l]}$ (plain big NN)\n",
    "* $x$ -> |big NN| -> $a^{[l]}$ -> FC1 -> FC2 -> $a^{[l+2]}$ (res-NN)\n",
    "    * skip connection between $a^{[l]}$ & $a^{[l+2]}$\n",
    "    * ReLU activations, this $a \\geq 0$\n",
    "    * $a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$ (skip connection)\n",
    "    * $g(z^{[l+2]}+a^{[l]}) = g(W^{[l+2]}\\cdot a^{[l+1]} + b^{[l+2]} + a^{[l]})$\n",
    "        * if weight decay is applied it might lead to making $W^{[l+2]}$ and $b^{[l+2]}$ close to zero\n",
    "        * thus $a^{[l+2]} = g(a^{[l]}) = a^{[l]}$\n",
    "        * identity function is easy to learn with residual block!\n",
    "    * for the core operation $a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$ to work, $z^{[l+2]}$ and $a^{[l]}$ must have same dimensions, thus same padding is used in CNN layers, or scaling matrix is added to modify the operation to $a^{[l+2]} = g(z^{[l+2]}+W_{s}\\cdot a^{[l]})$, where scaling matrix $W_{s}$ can be learned or fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception\n",
    "\n",
    "**Inception building block - network in network**\n",
    "* [paper](https://arxiv.org/pdf/1312.4400)\n",
    "* (1 x 1) convolution\n",
    "* for 1 channel and same filter sizes, it just scales the original channel\n",
    "* for multiple channel input (6 x 6 x 32) with filter (1 x 1 x 32) and activation, output would be (6 x 6 x 32)\n",
    "    * this be perceived as full-connection across channels with activation applied (this is why network in network)\n",
    "* useful for shrinking number of filters (as opposed to filter size reduced by pooling)\n",
    "* for other cases (same number of channels), it just adds non-linearity and allows for more complex patterns\n",
    "\n",
    "**Motivation**\n",
    "* [paper](https://arxiv.org/pdf/1409.4842)\n",
    "* decisions about NN architecture are complicated, why not to use every operation at the same time\n",
    "\n",
    "**Intuition example**\n",
    "* inception block\n",
    "    * input (28 x 28 x 192)\n",
    "    * stacking multiple blocks together\n",
    "        * 64 filters with size (1 x 1), stride 1, output (28 x 28 x 64)\n",
    "        * 128 filters with size (3 x 3), stride 1, same padding, output (28 x 28 x 128)\n",
    "        * 32 filters with size (5 x 5), stride 1, same padding, output (28 x 28 x 32)\n",
    "        * max-pool layer, stride 1, same padding, output (28 x 28 x 32)\n",
    "    * output (28 x 28 x 256)\n",
    "\n",
    "* computational cost example\n",
    "    * input (28 x 28 x 192), 32 filters with size (5 x 5), stride 1, same padding, output (28 x 28 x 32)\n",
    "    * 32 filters of size 5 * 5 * 192, with 28 * 28 * 32 multiplications, thus ~120m operations\n",
    "    \n",
    "* optimized example\n",
    "    * input (28 x 28 x 192)\n",
    "    * 16 filters with size (1 x 1), output (28 x 28 x 16)\n",
    "    * 32 filters with size (5 x 5), stride 1, same padding, output (28 x 28 x 32)\n",
    "    * computational costs 28 * 28 * 16 * 192 ~ 2.4m, 28 * 28 * 32 * 5 * 5 *16 ~ 10m, thus ~12.4m operations\n",
    "\n",
    "\n",
    "* inception block optimized\n",
    "    * input (previous activation), with dimensions (28 x 28 x 192)\n",
    "    * 96 conv filters with size (1 x 1), 128 conv filters with size (28 x 28), stride 1 and same padding\n",
    "    * 16 conv filters with size (1 x 1), 32 conv filters with size (5 x 5), stride 1 and same padding\n",
    "    * 64 conv filters with size (1 x 1) \n",
    "    * max-pool layer with filter (3 x 3), stride 1, same padding, 32 conv filters with size (1 x 1), that is output is (28 x 28 x 32)\n",
    "    * output concats the previous steps and has size (28 x 28 x 256), this layer also called channel concat\n",
    "\n",
    "* architecture\n",
    "    * just chaining inception blocks\n",
    "    * side branches\n",
    "        * branch of network after inception block\n",
    "        * fc layer\n",
    "        * output layer for label prediction (softmax)\n",
    "        * regularizing effect (?)\n",
    "\n",
    "Summary  \n",
    "* sometimes called *googLeNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## MobileNet\n",
    "\n",
    "* CNNs can be computationally intensive\n",
    "* normal convolution\n",
    "    * input (n x n x n_c) * filter (f x f x n_c') = output (n_out x n_out x n_c') [no padding, stride 1]\n",
    "    * number of multiplications (2160) -> #filter params * #filter positions * #number of filters\n",
    "\n",
    "* depth-wise separable convolution\n",
    "    * depth-wise convolution\n",
    "        * input (n x n x n_c) * depth filter (f x f x n_c) based on channels = output (n_out x n_out x n_c)\n",
    "        * number of multiplications (432) -> #filter params * #filter position x #number of filters\n",
    "    * point-wise convolution\n",
    "        * output (n_out x n_out x n_c) * point filter (1 x 1 x n_c') = (n_out x n_out x n_c')\n",
    "        * number of multiplications (240) -> #filter params * #filter position x #number of filters\n",
    "    * reduction in computational cost by factor of ~ 3 for the example at hand, for general case can be estimated as $\\frac{1}{n_c'}+\\frac{1}{f^2}$\n",
    "* inference step much more efficient!\n",
    "\n",
    "\n",
    "**Architecture**\n",
    "\n",
    "* using separable convolution (depth-wise and point-wise operations) instead of expensive traditional convolution operation\n",
    "* [paper v2](https://arxiv.org/pdf/1801.04381)\n",
    "\n",
    "* building blocks (v1)\n",
    "    * input layer\n",
    "    * separable convolution (13 layers)\\\n",
    "    * pooling\n",
    "    * fully-connected layer\n",
    "    * output layer (softmax activation)\n",
    "\n",
    "* building blocks (v2)\n",
    "    * input layer\n",
    "    * separable convolution (17 layers, \"bottleneck block\")\n",
    "        * residual connection (more efficient gradient propagation)\n",
    "        * expansion layer\n",
    "        * projection layer\n",
    "    * pooling\n",
    "    * fully-connected layer\n",
    "    * output layer (softmax activation)\n",
    "\n",
    "\n",
    "* v2 bottleneck block\n",
    "    * structure\n",
    "        * input (n x n x 3)\n",
    "        * expansion layer (1 x 1 x n_c), where n_c somewhat large (ie 6 times channels from previous layer)\n",
    "        * depth-wise convolution (n x n x n_c), same padding\n",
    "        * point-wise convolution (n x n x 3), sometimes called projection step (projecting down from the depth-wise convolution)\n",
    "        * residual conection to the previous layer\n",
    "    * enables to learn richer functions\n",
    "    * helps with memory needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Efficient net\n",
    "\n",
    "* [paper](https://arxiv.org/pdf/1905.11946)\n",
    "* the goal is to scale the NN to the (edge) device\n",
    "* baseline architecture\n",
    "    * input size\n",
    "    * depth\n",
    "    * width\n",
    "* efficient architecture\n",
    "    * higher/lower resolution image \n",
    "    * higher/lower depth\n",
    "    * higher/lower width\n",
    "* what are sensible decision wrt the NN architecture's params? what is the best trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Practical advice\n",
    "\n",
    "* open-source implementations benefit from reusing existing building blocks, and exploring implementation details of particular solutions, they are usually published on Github, pre-train nets are great\n",
    "* transfer learning (freezing/training weights in layers), can be split to two steps (pre-computation in the feature extraction piece and saving the outcomes to disc, training shallow classification net on the results)\n",
    "* data augmentation might help, when not enough data\n",
    "    * mirroring/flipping along an axe, random cropping, rotation, shearing, local warping\n",
    "    * color shifting (distorting channels, PCA approach in the AlexNet paper)\n",
    "    * generating distortion on CPU in parallel during mini-batch generation for training\n",
    "* CV\n",
    "    * data vs hand-engineering (object detection (not much data)-> speech recognition (lots of data))\n",
    "        * with lots of data -> simple algorithms, less hand-engineering\n",
    "        * with not much data -> hand-engineering features, architecture, other components, hacks; transfer learning\n",
    "    * hacks\n",
    "        * ensembling\n",
    "        * multi-crop at test time (producing larger validation sets, ie 10-crop)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
