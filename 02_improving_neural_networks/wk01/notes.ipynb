{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning setup\n",
    "\n",
    "* quickly interating over idea, code and experiments\n",
    "* experimental design important (train/dev/test splits & eval)\n",
    "    * when n observations in ~ 10.000, traditional splits (70/30 or 60/20/20)\n",
    "    * when n observations in ~ 10^6, dev/test sets might be in a ballpark of ~ 10.000\n",
    "    * train-dev-test distributions might be mismatched\n",
    "\n",
    "**Bias/Variance**\n",
    "\n",
    "* high bias (underfitting the data), just right, high variance (overfitting the data)\n",
    "* train set error vs dev set error\n",
    "    * train set error high, dev set error high -> underfitting (high bias)\n",
    "    * train set error low, dev set error high -> overfitting (high variance)\n",
    "    * train set error high, dev set error very high -> high bias and high variance, wrong and detailed decision boundary (!)\n",
    "    * and optimistic option (low bias & variance)\n",
    "\n",
    "**The recipe**\n",
    "\n",
    "* Does the algorithm have high bias? (train performance)\n",
    "    * bigger network\n",
    "    * train longer\n",
    "    * use different optimization algorithms\n",
    "    * different architecture\n",
    "    * experiment until acceptable bias achieved\n",
    "\n",
    "* Does the algorithm have high variance? (dev performance)\n",
    "    * more data\n",
    "    * regularization\n",
    "    * different architecture\n",
    "    * experiment until acceptable variance achieved\n",
    "\n",
    "Bias-variance trade-off, in previous ML era the trade-off between the two was present, this is just not valid for NNs (can reduce both)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "**L2**  \n",
    "\n",
    "* helping with high variance, hyperparameter $\\lambda$ (strenght of the regularization)\n",
    "* for logreg cost function $J(w,b)$ in regularization scenarios with L2 reg $J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)},y^{(i)}) + \\frac{\\lambda}{2m}\\big\\| w\\big\\|_2^2$\n",
    "* bias term usually omitted\n",
    "* L2 reg -> $\\big\\| w\\big\\|_2^2 = \\sum_{j=1}^{n_x} w_j^2 = w^T w$, most commonly used\n",
    "* L1 reg -> $\\big\\| w\\big\\|_1 = \\sum_{j=1}^{n_x}|w_j|$, making model sparse\n",
    "\n",
    "* in neural net cost function $J(w^{[1]},b^{[1]}, ..., w^{[l]},b^{[l]}) = \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)},y^{(i)}) + \\frac{\\lambda}{2m}\\big\\| w^{[l]}\\big\\|^2$\n",
    "* $\\big\\| w^{[l]}\\big\\|^2 = \\sum_{i=1}^{n^{[l]}} \\sum_{j=1}^{n^{[l-1]}} (w_{i,j}^{[l]})^2$, where $i$ describes the number of neurons in the current layer, $j$ equal to the number of neurons from previous layer (this is due to the dimensions of $w^{[l]}$ matrix), it is called Frobenius norm\n",
    "* in backprob $dW^{[l]} = (from \\ bp) + \\frac{\\lambda}{m}w^{[l]}$\n",
    "* update params $W^{[l]} = W^{[l]} - \\frac{\\alpha\\lambda'}{m}W^{[l]} - \\alpha (from \\ bp)$\n",
    "* where the first piece can be simplified to $(1-\\frac{\\alpha\\lambda}{m}W^{[l]})$, thus L2 is also called weight decay (as we are decaying the weigt matrices)\n",
    "\n",
    "* by setting $\\lambda$ to large values, one is reducing impact of internal units (effectively removing them), thus making the network simple\n",
    "* another example might be network with tanh activation func, where forcing weigts to be small leads to output of the unit in linear piece of the function, thus making the output of the network closer to linear func (NN w/o non-linear activations can output just linear func)\n",
    "\n",
    "**Dropout**\n",
    "\n",
    "* setting probability of removing a node from a layer, resulting in simplified version of the network\n",
    "* constructing the temp architecture on randomly for each example\n",
    "* step-by-step forward pass of \"inverted dropout\"\n",
    "    * set probability for keeping the unit\n",
    "    * generate array of random numbers, 1 if random > keep prob, 0 otherwise\n",
    "    * multiply activation result with the boolean vector\n",
    "    * scale up the total result of activation back (divide by keep prob) -> \"inverted dropout\" to keep comparable expected values\n",
    "* used only during training, not in inference part\n",
    "\n",
    "* drop-out force the network to be more robust (cannot rely on any one features), effectively spreading out weights (similar to regularization!)\n",
    "* keep prob can be set out based on layers, ie lower on larger weight matrices\n",
    "\n",
    "**Others**\n",
    "\n",
    "* data augmentation - ie flipping, rotating, cropping and applying other transformations to images,\n",
    "* early stopping - stopping when dev error not improving, not conceptual clean (chasing more problems simultaneously)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Optimization\n",
    "\n",
    "* input normalization\n",
    "    * z-scaling, minmax\n",
    "    * improves symmetry in the optimization problem\n",
    "\n",
    "* vanishing/exploding gradients\n",
    "    * for deep nets, by multiplication gradients can vanish or explode exponentialy \n",
    "    * vanish if W<I, explode if W>I, where I -> identity matrix\n",
    "\n",
    "* weight initialization\n",
    "    * a lot of inputs -> smaller weights are good\n",
    "    * for relu, setting the variance to $\\frac{2}{n}$ helps (for internal units, this is generally based on the no of units in prev layer)\n",
    "    * for tanh, we set variance to $\\frac{1}{n}$\n",
    "    * some other approaches might be considered\n",
    "\n",
    "* numerical approximation of gradients\n",
    "    * using larger two-side intervals for numerical check of the derivation (in classic case, we would use just one-sided approach)\n",
    "    * $\\frac{f(\\theta+\\epsilon)-\\theta-\\epsilon)}{2\\epsilon} \\approx g(\\theta)$\n",
    "\n",
    "* gradient checking\n",
    "    * helps with implementation\n",
    "    * concat every param vec into large matrix $\\theta$\n",
    "    * concat gradient vecs into large matrix $d\\theta$\n",
    "    * for each $\\theta$ get:\n",
    "        * $d\\theta_{approx} [i] = \\frac{J(\\theta_1, \\theta_2, ... , \\theta_i +\\epsilon, ...)-J(\\theta_1, \\theta_2, ... , \\theta_i -\\epsilon, ...)}{2\\epsilon} \\\\ \\approx d\\theta[i] = \\frac{\\delta J}{\\delta{\\theta_i}}$\n",
    "    * compute euclidean distance between the estimated and expected grads, epsilon on scale of 10^-7, distance < 10^-5 ok, <10^-3 bad\n",
    "\n",
    "* gradient checking implementation notes\n",
    "    * use only to debug (computationally costly)\n",
    "    * after grad check fails, check on the components (particular derivatives, layers)\n",
    "    * remember regularization term\n",
    "    * does not work with dropout (turn off dropout for debug, set keep_prob=1.0)\n",
    "    * check after initialization and after some training"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
