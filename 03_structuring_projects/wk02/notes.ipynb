{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning strategy\n",
    "\n",
    "## Error analysis\n",
    "\n",
    "* estimate impact of particular problem to the system (upper bound on performance improvement) to guide further effort\n",
    "* evaluate multiple ideas in parallel (hypothesis about model failure)\n",
    "* rank/prioritize ideas based on impact\n",
    "\n",
    "* incorrectly labeled data\n",
    "    * systematic errors are problematic random errors are ok (if the dataset large enough)\n",
    "    * treat it as other error causes (estimate impact and prioritize with other problems)\n",
    "    * probably check some correctly classified examples\n",
    "    * fix both dev & test sets\n",
    "\n",
    "* how to start\n",
    "    * set up experiment & eval metric quickly\n",
    "    * iterate\n",
    "    * error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Mismatched splits\n",
    "\n",
    "* align dev & test splits based on your target goal (might differ from train set)\n",
    "* bias & variance on mismatched splits\n",
    "    * train-dev set (piece of train set not used for training but from same distribution as train)\n",
    "        * if error high, solution is over-fitted\n",
    "        * if error low, probably data mismatch problem\n",
    "            * manual error analysis\n",
    "            * make splits more similar (augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Multiple tasks\n",
    "\n",
    "* transfer learning -> repurposing existing NN, as lower-level features can be shared across problems with same inputs\n",
    "    * pretraining -> whole net retrained\n",
    "    * fine-tuning -> only last layers retrained\n",
    "    * lot of data for the initial training (pics, sounds, ...), little data for the problem at hand (radiology, speech recognition, ...)\n",
    "\n",
    "* multi-task learning -> simultaneously working on multiple problems (ie multi-label classification)\n",
    "    * wide output layer (or multiple heads)\n",
    "    * loss distributed across the problems\n",
    "    * problems with shared low-level features\n",
    "    * problem representation similar, may improve data size\n",
    "    * one global model to do well on the most of the tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## E2E deep learning\n",
    "\n",
    "* multiple steps in a project\n",
    "    * speech\n",
    "        * audio -> features -> phonemes -> words -> transcript\n",
    "        * audio ->->-> (large NN) transcript\n",
    "            * vast amount of data needed\n",
    "    * translation \n",
    "        * English -> text analysis -> ... -> French\n",
    "        * English ->->-> French\n",
    "\n",
    "    * estimating child's age\n",
    "        * image -> bones -> age\n",
    "        * image ->->-> age\n",
    "\n",
    "    * other examples\n",
    "        * identification of a person\n",
    "            * detect person's face, crop the face\n",
    "            * use cropped face to detect person's identity\n",
    "            * lot of data for both sub-tasks\n",
    "* summary\n",
    "    * pros\n",
    "        * let data speak\n",
    "        * less hand-designing components needed\n",
    "    * cons\n",
    "        * large amount of data needed\n",
    "        * excludes hand-design components (needed when hand-design features needed)\n",
    "    * Do we have enough data to learn a function of the complexity needed to map x to y?\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
