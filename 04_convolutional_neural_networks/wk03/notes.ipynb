{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection\n",
    "\n",
    "## Object localization\n",
    "* classification\n",
    "    * label\n",
    "    * one object\n",
    "* classification with localization\n",
    "    * label AND bounding box\n",
    "    * one object\n",
    "* detection\n",
    "    * labels AND bounding boxes\n",
    "    * multiple objects\n",
    "\n",
    "* solution\n",
    "    * adding output units for bounding box\n",
    "        * $b_x, b_y, b_h, b_w$\n",
    "        * middle point, height and with of the box\n",
    "        * convention is that these are defined in terms of input image size\n",
    "    * target vector\n",
    "        * is there an object?\n",
    "        * bounding box\n",
    "        * classes\n",
    "        * examples $y = \\begin{bmatrix} p_c \\\\ b_x \\\\ b_y \\\\ b_h \\\\ b_w \\\\ c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix}$, $\\begin{bmatrix} 1 \\\\ b_x \\\\ b_y \\\\ b_h \\\\ b_w \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $\\begin{bmatrix} 0 \\\\ ? \\\\ ? \\\\ ? \\\\ ? \\\\ ? \\\\ ? \\\\ ? \\end{bmatrix}$\n",
    "    * example of squared loss\n",
    "        * $ L(\\hat y,y) = (\\hat y_1-y_1)^2+(\\hat y_2-y_2)^2+...+(\\hat y_8-y_8)^2$, if $y_1=1$\n",
    "        * $L(\\hat y,y) = (\\hat y_1-y_1)^2$, if $y_1=1$\n",
    "        * different types of losses across the elements\n",
    "\n",
    "* landmark detection\n",
    "    * detect multiple important points of the image (ie persons image and eye position and silhouette, pose detection)\n",
    "    * output is object there and point coordinates\n",
    "    * labels need to be consistent across images\n",
    "\n",
    "## Object detection\n",
    "* sliding window algorithm\n",
    "    * algorithm\n",
    "        * looping window through a image using a sliding window\n",
    "        * classifying whether there is an object in the sliding window\n",
    "        * pick larger sliding window\n",
    "        * looping window through the image using the new window\n",
    "        * classifying whether there is the object\n",
    "        * ...\n",
    "    * computationally intensive\n",
    "    * can be implemented convolutionally!\n",
    "* conv sliding window algorithm\n",
    "    * [paper](https://arxiv.org/abs/1312.6229)\n",
    "    * conv net\n",
    "        * input (14 x 14 x 3)\n",
    "        * conv layer (5 x 5) with 16 filters\n",
    "        * max pool layer (2 x 2)\n",
    "        * FC with 400 units\n",
    "        * FC with 400 units\n",
    "        * output layer with softmax\n",
    "    * transforming FC layer to conv\n",
    "        * input (14 x 14 x 3)\n",
    "        * conv layer (5 x 5) with 16 filters\n",
    "        * max pool layer (2 x 2)\n",
    "        * conv layer (5 x 5) with 400 filters, output (1 x 1 x 400)     \n",
    "        * conv layer (1 x 1) with 400 filters, output (1 x 1 x 400)\n",
    "        * conv layer (1 x 1) with 4 filters, output fed into softmax\n",
    "    * further example (base)\n",
    "        * input (14 x 14 x 3)\n",
    "        * conv layer 16 filters (5 x 5), output (10 x 10 x 16), stride 1, no padding\n",
    "        * max pool (2 x 2), output (5 x 5 x 16)\n",
    "        * conv layer 400 filters (5 x 5), output (1 x 1 x 400)\n",
    "        * conv layer 400 filters (1 x 1), output (1 x 1 x 400)\n",
    "        * conv layer 4 filters (1 x 1), softmax on top, output (1 x 1 x 4)\n",
    "    * further example (extended)\n",
    "        * input (16 x 16 x 3), slightly larger image with 4 sliding windows (14 x 14 x 3), with stride 2\n",
    "        * conv layer 16 filters (5 x 5) with stride 1, output (12 x 12 x 16)\n",
    "        * max pool (2 x 2), output (6 x 6 x 16)\n",
    "            * this step is responsible for the stride of 2 in the original image\n",
    "        * conv layer 400 filters (5 x 5), output (2 x 2 x 400)\n",
    "        * conv layer 400 filters (2 x 2), output (2 x 2 x 400)\n",
    "        * conv layer 4 filters (2 x 2), softmax on top, output (2 x 2 x 4)\n",
    "            * every point in the layer corresponds to the corner fo the input image (ie upper left)\n",
    "\n",
    "## Bounding box predictions\n",
    "* YOLO (You only look once) [paper](https://arxiv.org/abs/1506.02640)\n",
    "* detects object midpoint within the image grid + bounding box around the object\n",
    "* target again $p_c$, $b_x$, $b_y$, $b_h$, $b_w$, $c_1$, $c_2$, $c_3$ (object, mid point, box, class)\n",
    "* example\n",
    "    * dividing input image into 3 x 3 grid, than target looks like (3 x 3 x 8), considering all definition above\n",
    "    * input (100 x 100 x 3)\n",
    "    * conv net (has to match i/o sizes)\n",
    "    * output (3 x 3 x 8)\n",
    "* it is sensible to use finer grids to avoid multiple objects in one window\n",
    "* bounding boxes might use specific activations to ensure valid ranges\n",
    "\n",
    "## Intersection over union\n",
    "* used for evaluating object localization, prediction vs ground truth\n",
    "* (size of the intersection of prediction and ground truth) / (the size of the union of those), \"correct\" if IoU >= 0.5\n",
    "\n",
    "## Non-max suppression\n",
    "* multiple detection of the same object\n",
    "* results in multiple midpoints, multiple bounding boxes\n",
    "* high-level intuition\n",
    "    * $p_c$ with highest probability of the object (lets highlight that)\n",
    "    * non-max suppression looks at detections with high overlap (high IoU), and suppresses them\n",
    "    * then it continues with the following highest $p_c$ detected and suppresses non-max predictions with high IoU\n",
    "    * ...\n",
    "* algorithm (for a single class)\n",
    "    * discard all predictions with $p_c \\leq 0.6$, deal only with the high-confident onces\n",
    "    * while there are remaining boxes:\n",
    "        * pick the box with the highest $p_c$, output that as a prediction\n",
    "        * discard any remaining box with IoU >= 0.5 with the box output in the previous step\n",
    "\n",
    "## Anchor boxes\n",
    "* addresses the problem of multiple objects in one grid cell\n",
    "* multiple anchor boxes (shaped bounding boxes) can be selected\n",
    "* forces network to specialize\n",
    "* target comprises then of multiple anchors put on top of each other such as $y = \\begin{bmatrix} p^1_c \\\\ b^1_x \\\\ b^1_y \\\\ b^1_h \\\\ b^1_w \\\\ c^1_1 \\\\ c^1_2 \\\\ c^1_3 \\\\ p^2_c \\\\ b^2_x \\\\ b^2_y \\\\ b^2_h \\\\ b_w \\\\ c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix}$, where upper indices marks anchors\n",
    "* previously\n",
    "    * each object in image assigned to grid cell that contains the midpoint\n",
    "    * target vector (3 x 3 x 8)\n",
    "* with 2 anchor boxes\n",
    "    * each object in image is assigned to cell with the midpoint and anchor box for the cell with the highest IoU (grid cell, anchor box)\n",
    "    * target vector (3 x 3 x 16)\n",
    "    * does not work well with 3 objects, same shape object also problematic\n",
    "\n",
    "## YOLO algorithm\n",
    "\n",
    "* train set construction\n",
    "    * 3 classes, 2 bounding boxes, img grid 3 x 3\n",
    "    * y is 3 x 3 (grid) x 2 (anchor boxes) x 8 (bounding boxes 5 + number of classes 3)\n",
    "    * for each grid cell, y is constructed, if the object is present, bounding box based on IoU is selected\n",
    "    * image input -> volume block output\n",
    "\n",
    "* making predictions\n",
    "    * where $p_c = 0$, than other values ignored\n",
    "    * where $p_c = 1$, than correct values across the board\n",
    "    * algorithm\n",
    "        * for each grid cell, predict 2 bounding boxes\n",
    "        * get rid of low-prob predictions\n",
    "        * for each class separately, generate non-max suppression\n",
    "\n",
    "## Region proposal\n",
    "\n",
    "* regions with CNNs, the goal is to pick just a few CNN windows to run through CNN (ignores empty regions)\n",
    "* do image segmentation, place bounding boxes around interesting regions, feed the regions to CNN\n",
    "* R-CNN [paper](https://arxiv.org/pdf/1311.2524)\n",
    "    * propose regions, classify regions one at a time, output label & bounding box\n",
    "* Fast R-CNN [paper](https://arxiv.org/pdf/1504.08083)\n",
    "    * propose regions, use conv implementation of sliding windows to classify proposed regions\n",
    "* Faster R-CNN [paper](https://arxiv.org/pdf/1506.01497)\n",
    "    * initial segmentation still slow, using CNN to propose regions\n",
    "\n",
    "\n",
    "## Semantic segmentation\n",
    "\n",
    "* draw an outline around the detected object (pixel-wise)\n",
    "* per pixel labeling\n",
    "* architecture\n",
    "    * input image\n",
    "    * conv & pool layers (shrinking height and with, expanding channels)\n",
    "    * conv & pool layers (expanding heigh and with, shrinking channels)\n",
    "\n",
    "### Transpose convolution\n",
    "\n",
    "* enables for blowing up inputs\n",
    "* traditional conv (6 x 6 x 3) image, convolved with 5 filters of (3 x 3 x 3), resulting in (4 x 4 x 5)\n",
    "* transpose conv (2 x 2) image, convolved with (3 x 3), resulting in (4 x 4)\n",
    "    * padding p = 1, stride s = 2\n",
    "    * upper left entry of the input, convolved with the filter, pasting result in the upper left corner, padding left unfilled,\n",
    "    * moving to the second value in the input, multiply with the filter, project to the output, padding set to zero, add values where there are overlaps with the previous operation, input values to remaining positions\n",
    "    * continue with the remaining two positions in the input using same process\n",
    "\n",
    "### U-net\n",
    "\n",
    "* [paper](https://arxiv.org/pdf/1505.04597)\n",
    "* architecture intuition\n",
    "    * input image\n",
    "    * traditional convolution (detailed spatial info lost)\n",
    "    * transpose convolution\n",
    "    * skip connection from initial conv layers to the last conv layers (bringing detailed spatial info)\n",
    "    * output layer\n",
    "\n",
    "* architecture overview\n",
    "    * input image (h x w x 3)\n",
    "    * conv layer & ReLU\n",
    "    * conv layer & ReLU\n",
    "    * max-pooling to reduce h & w\n",
    "        * conv layer & ReLU\n",
    "        * conv layer & ReLU  \n",
    "        * max-pooling to reduce h & w      \n",
    "            * conv layer & ReLU\n",
    "            * conv layer & ReLU  \n",
    "            * max-pooling to reduce h & w \n",
    "\n",
    "                * trans conv layer & skip connects from prev layer\n",
    "            * conv layer & ReLU\n",
    "            * conv layer & ReLU\n",
    "            * trans conv layer & skip connects from prev layer\n",
    "        * conv layer & ReLU\n",
    "        * conv layer & ReLU\n",
    "        * trans conv layer & skip connects from prev layer                \n",
    "    * conv layer & ReLU\n",
    "    * conv layer & ReLU\n",
    "    * trans conv layer & skip connects from prev (first) layer\n",
    "    * conv layer & ReLU\n",
    "    * conv layer & ReLU\n",
    "    * (1 x 1) output layer, output (h x w x no of classes), argmax over classes dimension\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
