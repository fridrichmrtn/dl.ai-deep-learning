{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case studies\n",
    "\n",
    "## Classic networks\n",
    "\n",
    "**LeNet5**\n",
    "* [paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n",
    "* goal is to recognize hand-written digit from gray-scale image\n",
    "* building blocks\n",
    "    * input layer (32 x 32 x 1)\n",
    "    * CNN1\n",
    "        * 6 filters with size (5 x 5), stride 1 and no padding\n",
    "        * output (28 x 28 x 6)\n",
    "        * average-pooling, filter (2 x 2), stride 2\n",
    "        * output (14 x 14 x 6)\n",
    "    * CNN2\n",
    "        * 16 filters with size (5 x 5), stride 1 and no padding\n",
    "        * output (10 x 10 x 16)\n",
    "        * average-pooling, filter (2 x 2), stride 2\n",
    "        * output (5 x 5 x 16)\n",
    "        * flatten to 400 units\n",
    "    * FC1\n",
    "        * 120 units\n",
    "    * FC2\n",
    "        * 84 units\n",
    "    * output layer\n",
    "        * 10 units\n",
    "        * softmax activation\n",
    "        * original implementation used different approach -> euclidean radial basis function\n",
    "\n",
    "Summary  \n",
    "* 60k params\n",
    "* with depth, $n_h$, $n_w$ decreases, $n_c$ increases\n",
    "* conv + pool, conv + pool, fc & fc & output\n",
    "* sigmoid & tanh no ReLU\n",
    "\n",
    "**AlexNet**\n",
    "* [paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "* building blocks\n",
    "    * input layer (227 x 227 x 3)\n",
    "    * CNN1\n",
    "        * 96 filters with size (11 x 11), stride 4\n",
    "        * output (55 x 55 x 96)\n",
    "        * max-pooling, filter (3 x 3), stride 2\n",
    "        * output (27 x 27 x 96)\n",
    "    * CNN2\n",
    "        * 256 filters with size (5 x 5), same padding\n",
    "        * output (27 x 27 x 256)\n",
    "        * max-pooling, filter (3 x 3), stride 2\n",
    "        * output (13 x 13 x 256)\n",
    "    * CNN3-5\n",
    "        * 384 filters with size (3 x 3), same padding\n",
    "        * output (13 x 13 x 384)\n",
    "        * 384 filters with size (3 x 3), same padding\n",
    "        * output (13 x 13 x 384)\n",
    "        * 256 filters with size (3 x 3), same padding\n",
    "        * output (13 x 13 x 256)\n",
    "        * max-pooling, filter (3 x 3), stride 2\n",
    "        * output (6 x 6 x 256)\n",
    "        * flatten to 9216 units\n",
    "    * FC1\n",
    "        * 4096 units\n",
    "    * FC2\n",
    "        * 4096 units\n",
    "    * output layer\n",
    "        * 1000 units\n",
    "        * softmax activation\n",
    "\n",
    "Summary  \n",
    "* 60m params\n",
    "* large ImageNet dataset\n",
    "* ReLU activations\n",
    "* distributed training on multiple GPU\n",
    "* local response normalization layer (normalization across filter position), this does not seem to help much\n",
    "\n",
    "**VGG16**\n",
    "* [paper](https://arxiv.org/pdf/1409.1556)\n",
    "* 16 layers of params\n",
    "* building blocks\n",
    "    * input layer (224 x 224 x 3)\n",
    "    * CNN1-2\n",
    "        * 64 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (224 x 224 x 3)\n",
    "        * 64 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (224 x 224 x 64)\n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (112 x 112 x 64)\n",
    "    * CNN3-4\n",
    "        * 128 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (112 x 112 x 128)\n",
    "        * 128 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (112 x 112 x 128)\n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (56 x 56 x 128)\n",
    "    * CNN5-7\n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (56 x 56 x 256)       \n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (56 x 56 x 256)     \n",
    "        * 256 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (56 x 56 x 256)\n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (28 x 28 x 256)\n",
    "    * CNN8-11\n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (28 x 28 x 512)              \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (28 x 28 x 512)            \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (28 x 28 x 512)    \n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (14 x 14 x 512)\n",
    "    * CNN12-14\n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (14 x 14 x 512)              \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (14 x 14 x 512)            \n",
    "        * 512 filters with size (3 x 3), stride 1, same padding\n",
    "        * output (14 x 14 x 512)    \n",
    "        * max-pooling, filter (2 x 2), stride 2\n",
    "        * output (7 x 7 x 512)\n",
    "        * flatten to 25088 units\n",
    "    * FC1-2\n",
    "        * 4096 units\n",
    "        * ReLU activation\n",
    "        * 4096 units\n",
    "        * ReLU activation\n",
    "    * output layer\n",
    "        * 1000 units\n",
    "        * softmax activation\n",
    "\n",
    "Summary  \n",
    "* 138m params\n",
    "* uniform (symmetrical?) architecture\n",
    "* with depth, $n_h$, $n_w$ decreases by factor of 2, $n_c$ increases by factor of 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ResNet\n",
    "\n",
    "* [paper](https://arxiv.org/pdf/1512.03385)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
