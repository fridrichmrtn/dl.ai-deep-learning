{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Recurrent neural networks\n",
    "\n",
    "* sequence problems\n",
    "    * speech recognition (sound -> transcription)\n",
    "    * music generation (genre -> )\n",
    "    * sentiment classification (sentence -> sentiment)\n",
    "    * DNA sequence analysis\n",
    "    * machine translation (lang1 -> lang2)\n",
    "    * video activity recognition (video frames -> activity)\n",
    "    * name entity recognition (sentence -> persons)\n",
    "* notation\n",
    "    * x (sentence), y (binary vector with 1 where the word is part of a name)\n",
    "    * word in a sequence from $x^{<1>}$ to $x^{<t>}$, where $t$ is the number of tokens\n",
    "    * scalar in a target sequence from $y^{<1>}$ to $y^{<t>}$\n",
    "    * for $i$ example and its $t$ word we use notation $x^{(i)<t>}$, similarly for target vector\n",
    "* vocabulary/dictionary\n",
    "    * tokens/words used to build a model\n",
    "    * one-hot representation can be used for encoding the word within the sentence\n",
    "\n",
    "## Recurrent neural network model\n",
    "\n",
    "* using standard feed-forward NN\n",
    "    * does not shared features learned across positions\n",
    "    * inputs and outputs can be different lengths\n",
    "* architecture\n",
    "    * $x^{<1>}$ -> NN -> $\\hat y^{<1>}$\n",
    "    * $x^{<2>}$ -> NN + $a^{<1>}$ -> $\\hat y^{<2>}$, activation from previous step passed\n",
    "    * $x^{<3>}$ -> NN + $a^{<2>}$ -> $\\hat y^{<3>}$, activation from previous step passed\n",
    "    * ...\n",
    "    * $x^{<t>}$ -> NN + $a^{<t-1>}$ -> $\\hat y^{<t>}$\n",
    "    * internal parameters are shared across steps for both vertical and horizontal connections\n",
    "    * connecting only earlier layers (addressed by bi-directional RNNs)\n",
    "* forward pass\n",
    "    * hidden activations tanh/ReLU, output activations sigmoid/softmax\n",
    "    * $a^{<0>} = 0$, $a^{<1>} = g(W_{aa}a^{<0>}+ W_{ax}x^{<1>}+b_a)$, $\\hat y^{<1>} = g(w_{ya}a^{<1>}+b_y)$\n",
    "    * $a^{<t>} = g(W_{aa}a^{<t-1>}+ W_{ax}x^{<t>}+b_a)$, $\\hat y^{<t>} = g(w_{ya}a^{<t>}+b_y)$\n",
    "    * can be simplified by horizontally stacking $W$ mats, and vertically stacking $x$ and $a$\n",
    "        * $a^{<t>} = g(W_a[a^{<t-1>},x^{<t>}]+b_a)$\n",
    "* backward pass\n",
    "    * $L^{<t>}(\\hat y^{<t>}, y^{<t>}) = -y^{<t>} \\log \\hat y^{<t>} - (1-y^{<t>}) \\log (1-\\hat y^{<t>})$\n",
    "    * $L(\\hat y^{<t>}, y^{<t>}) = \\sum_{i=1}^{T} L^{<t>}(\\hat y^{<t>}, y^{<t>}) $, that is loss sum over the seq\n",
    "    * partial derivates in opposite direction of the forward pass, recurrent path most significant (also called backprop through time)\n",
    "* more architectures\n",
    "    * one-to-one (standard MLP)\n",
    "    * many-to-many (described above)\n",
    "    * many-to-many (different lengths of x and y)\n",
    "        * encoder reads in the whole sentence (outputs starts at the end)\n",
    "        * decoder outputs the translation \n",
    "    * many-to-one (sentiment analysis)\n",
    "        * input layer -> sentence\n",
    "        * output layer -> sentiment\n",
    "        * architecture -> similar to the first one, output at the end of the sentence\n",
    "    * one-to-many (music generation)\n",
    "        * input layer -> genre (int)\n",
    "        * output layer -> set of notes\n",
    "        * architecture -> first input generates a note, that is fed into the next layer (incl prev activations), outputs another note, ...\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model\n",
    "\n",
    "* speech recognition\n",
    "    * P(the apple and pair salad) = 3.2 x 10^-13\n",
    "    * P(the apple and pear salad) = 5.7 x 10^-10\n",
    "* language model\n",
    "    * solving $P(y^{<1>},y^{<2>},...,y^{<Ty>}) = ?$\n",
    "    * training set: large corpus of text\n",
    "    * tokenize sentence into one-hot vector\n",
    "        * adding tokens describing start and end of sentence \\<EOS>\n",
    "        * adding tokens describing unknown tokens \\<UNK>\n",
    "* model\n",
    "    * inner workings\n",
    "        * input $x^{<t>} = y^{<t-1>}$\n",
    "        * Sentence \"Cats average 15 hours of sleep a day <EOS>\"\n",
    "        * $x^{<1>} = 0$ (Nothing) -> NN + $a^{<0>}$ -> $\\hat y^{<1>}$, where the output layer utilize softmax to predict probability of a word from the dictionary\n",
    "        * $x^{<2>} = y^{<1>}$ (Cats)-> NN + $a^{<1>}$ -> $\\hat y^{<2>}$, where the net outputs P(...|Cats)\n",
    "        * $x^{<3>} = y^{<2>}$ (average)-> NN + $a^{<2>}$ -> $\\hat y^{<3>}$, where the net outputs P(...|Cats average)\n",
    "        * ...\n",
    "        * $x^{<9>} = y^{<8>}$ (day)-> NN + $a^{<8>}$ -> $\\hat y^{<9>}$, where the net outputs P(<EOS>|...)\n",
    "    * cost function\n",
    "        * $L^{<t>}(\\hat y^{<t>}, y^{<t>}) = - \\sum_i y^{<t>} log(\\hat y^{<t>})$\n",
    "        * $L = \\sum_{t=1}^{Ty} L^{<t>}(\\hat y^{<t>}, y^{<t>})$\n",
    "    * resulting $p(y^{<1>}, y^{<2>}, y^{<3>}) = p(y^{<1>}) p(y^{<2>} | y^{<1>}) p(y^{<3>} | y^{<1>}, y^{<2>})$\n",
    "* sampling novel sequences\n",
    "    * word-level\n",
    "        * output of the language model are probabilities across the dictionary\n",
    "        * we can sample from the softmax layer using the outputs as probabilities and feed the sample further\n",
    "        * \\<UNK> samples rejected\n",
    "    * character-level\n",
    "        * similar to the approach above\n",
    "        * dont have to worry about \\<UNK>\n",
    "        * very long sentences/long range dependencies\n",
    "        * computationally intensive\n",
    "* vanishing gradient problem\n",
    "    * languages have long-term dependencies\n",
    "    * hard for backpropagation to deal with the earlier dependencies (initial words)\n",
    "    * local influence of tokens\n",
    "\n",
    "## Gated Recurrent Units (GRU)\n",
    "\n",
    "* [paper](https://arxiv.org/pdf/1409.1259)\n",
    "* [paper](https://arxiv.org/pdf/1412.3555)\n",
    "* traditional RNN\n",
    "    * computing activation at time t $a^{<t>} = g(W_a[a^{<t-1>},x^{<t>}]+b_a)$\n",
    "    * inputs $a^{<t-1>}$ , $x^{<t>}$\n",
    "    * outputs\n",
    "        * to the next unit $a^{<t>}$\n",
    "        * to the softmax $a^{<t>}$\n",
    "* GRU\n",
    "    * memory cells c, $c^{<t>} = a^{<t>}$\n",
    "    * candidate for replacing $c^{<t>}$, we obtain as $\\tilde c^{<t>} = tanh(W_c [c^{<t-1>, x^{<t>}}] + b_c)$\n",
    "    * update gate $\\Gamma_u = \\sigma(W_u [c^{<t-1>, x^{<t>}}] + b_u)$, with sigmoid activation func (result 1/0)\n",
    "    * gate deciding when to update the memory cell, memory cell propagating \"the memory\" further\n",
    "    * equation for updates $c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1-\\Gamma_u) * c^{<t-1>}$, that is if $\\Gamma_u = 1$ candidate value accepted, otherwise $\\Gamma_u = 0$  and old value kept\n",
    "    * structure of a unit\n",
    "        * inputs $a^{<t-1>} = c^{<t-1>}$, $x^{<t>}$\n",
    "        * internally $\\tilde c^{<t>}$, $\\Gamma_u$ calculated\n",
    "        * update/non-update memory cell with candidate values $\\tilde c^{<t>}$\n",
    "        * outputs \n",
    "            * to the next unit $c^{<t>} = a^{<t>}$\n",
    "            * to the softmax $a^{<t>}$\n",
    "    * the gate can store value through many layers, thus reduce vanishing gradient problem (can propagate activations from early layers almost exactly)\n",
    "    * size of $\\tilde c$, $c$, and $\\Gamma_u$ same, update is computed element-wise, allows to update some bits and keep other bits\n",
    "\n",
    "* Full GRU\n",
    "    * $\\tilde c^{<t>} = tanh(\\Gamma_r * W_c[c^{<t-1>},x^{<t>}]+b_c)$,\n",
    "        * $\\Gamma_r = \\sigma (W_r[c^{<t-1>}, x^{<t>}]+b_r)$, a gate which describes relevance of the new params\n",
    "    * $ \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}]+b_u)$\n",
    "    * $c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1-\\Gamma_u)*c^{<t-1>}$\n",
    "\n",
    "## Long Short Term Memory (LSTM)\n",
    "\n",
    "* [paper](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "* structure of a unit\n",
    "    * inputs $a^{<t-1>}$, $c^{<t-1>}$, $x^{<t>}$\n",
    "    * $\\tilde c^{<t>} = tanh(\\Gamma_r * W_c[a^{<t-1>},x^{<t>}]+b_c)$, change from $c^{<t-1>}$\n",
    "    * $ \\Gamma_u = \\sigma(W_u[a^{<t-1>}, x^{<t>}]+b_u)$, update gate\n",
    "    * $ \\Gamma_f = \\sigma(W_f[a^{<t-1>}, x^{<t>}]+b_f)$, forget gate\n",
    "    * $ \\Gamma_o = \\sigma(W_o[a^{<t-1>}, x^{<t>}]+b_o)$, output gate\n",
    "    * update $c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + \\Gamma_f * c^{<t-1>}$\n",
    "    * outputs $a^{<t>} = \\Gamma_o * tanh(c^{<t>})$, $c^{<t>}$\n",
    "* with chained units, we can propagate $c$ through the chain with many time steps\n",
    "* \"peephole connection\" is a variant where $c^{<t-1>}$ are propagated through the gates too\n",
    "\n",
    "## Others\n",
    "\n",
    "**Bidirectional RNN (BRNN)**  \n",
    "* looking back and forward using the RNN, traditionally we look only backwards\n",
    "* example\n",
    "    * $x^{<1>}$\n",
    "        * $\\overrightarrow a^{<1>}$, informs $\\overrightarrow a^{<2>}$\n",
    "        * $\\overleftarrow a^{<1>}$, based on $\\overleftarrow a^{<2>}$\n",
    "        * softmax out $y^{<1>}$, based on both units\n",
    "    * $x^{<2>}$\n",
    "        * $\\overrightarrow a^{<2>}$, informs $\\overrightarrow a^{<3>}$\n",
    "        * $\\overleftarrow a^{<2>}$, based on $\\overleftarrow a^{<3>}$\n",
    "        * softmax out $y^{<2>}$, based on both units\n",
    "    * ...\n",
    "* forms acyclic graph\n",
    "* blocks can be RNN, GRU or LSTM \n",
    "* $\\hat y^{<t>} = g(W_y [\\overrightarrow a^{<t>}, \\overrightarrow a^{<t>}] +b_y)$\n",
    "* whole sequence needed to produce prediction\n",
    "\n",
    "**Deep RNN**\n",
    "* horizontal & vertical propagation of the signal, same logic as in the shallow RNNs\n",
    "* usually less deep than other networks (computationally intensive due to temporal the connections)\n",
    "* possibly have RNN extraction connected to feed-forward layers /wo the temporal connections"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
