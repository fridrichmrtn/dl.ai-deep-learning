{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of CNN\n",
    "\n",
    "* used mostly in computer-vision problems such as image classification, object detection, style transfer, others\n",
    "* images may lead to large inputs (and consequently very large networks)\n",
    "\n",
    "## Convolution operation\n",
    "\n",
    "**Convolution**  \n",
    "* image (6 x 6) * filter (3 x 3) = result (4 x 4)\n",
    "    * (n x n) * (f x f) = (n-f+1 x n-f+1)\n",
    "        * f usually odd\n",
    "        * not full detection, image shrinks -> padding (adding pixels around)\n",
    "        * if p denotes padding size then the result size is (n+2p-f+1 x n+2p-f+1)\n",
    "        * valid convolution (no padding)\n",
    "            * (n x n) * (f x f) = (n-f+1 x n-f+1)\n",
    "        * same convolution (output size is same as original input size)\n",
    "            * (n+p x n+p) * (f x f) = (n+2p-f+1 x n+2p-f+1)\n",
    "            * p = (f-1)/2  \n",
    "* in some textbooks, filters are rotated (mirrored) before doing the transformation (to allow for associations), by convention this is not used in deep learning literature\n",
    "* filters can be learned through weights (!)        \n",
    "    \n",
    "Algorithm \n",
    "* sliding filter window into the image, multiplying filter with the image values, adding them together and putting them into the result matrix\n",
    "* sliding through the whole image from left to right (by 1 pixel), from top to down (by 1 pixel)\n",
    "* result matrix is filled, serves as filter detection (could be ie edge filter)\n",
    "* python: `conv-forward` tensorflow: `tf.nn.conv2d` keras: `tf.kearas.Conv2D`\n",
    "\n",
    "**Strides**\n",
    "* strides (movement by pixels) different from 1 can be used for the filter window\n",
    "* (n+p x n+p) * (f x f) = ([(n+2p-f)/s+1] x [(n+2p-f)/s+1])\n",
    "    * s is stride, if the fraction is not integer we round down\n",
    "\n",
    "**Volumes**\n",
    "* image (6 x 6 x 3) * filter (3 x 3 x 3) = result (4 x 4)  \n",
    "* detection across color channels (can look at one or all)\n",
    "* multiple filters can be applied at the same time resulting in stacking the 2d outputs (the last dimension of the result is driven by number of convolution filters)\n",
    "\n",
    "Algorithm\n",
    "* put filter into the starting position, do element-wise multiplication for     corresponding elements\n",
    "* sum the result and add it to the result matrix\n",
    "* move the filter by stride and repeat until the result matrix is filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One layer of CNN\n",
    "\n",
    "* number of params -> (filter size + bias) * number of filters\n",
    "* notation for layer $l$\n",
    "    * $m$ -> training examples\n",
    "    * $f^{[l]}$ -> filter size\n",
    "    * $p^{[l]}$ -> padding\n",
    "    * $d^{[l]}$ -> stride\n",
    "    * $n_c^{[l]}$ -> number of filters\n",
    "    * $f^{[l]}$ x $f^{[l]}$ x $n_c^{[l-1]}$ -> size of a filter, last dimension same as the number of channels in previous layer\n",
    "    * $f^{[l]}$ x $f^{[l]}$ x $n_c^{[l]}$ -> activations\n",
    "    * $m$ x $f^{[l]}$ x $f^{[l]}$ x $n_c^{[l]}$-> vectorized activations\n",
    "    * $f^{[l]}$ x $f^{[l]}$ x $n_c^{[l-1]}$ x $n_c^{[l]}$ -> weights\n",
    "    * $n_c^{[l]}$ -> bias\n",
    "\n",
    "* input -> $n_h^{[l-1]}$ x $n_w^{[l-1]}$ x $n_c^{[l-1]}$  \n",
    "* output -> $n_h^{[l]}$ x $n_w^{[l]}$ x $n_c^{[l]}$, where $n_h^{[k]}= [\\frac{n_w^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1]$, see less formally written version above\n",
    "\n",
    "* forward step (one filter)\n",
    "    * volume calculation (sliding, element-wise multiplication and sum)\n",
    "    * adding bias\n",
    "    * feeding the resulting matrix into an activation function\n",
    "    * stacking filters together (output of the convolution layer)\n",
    "    * ([(n+2p-f)/s+1] x [(n+2p-f)/s+1] x number of filters)\n",
    "\n",
    "* backward step ?\n",
    "\n",
    "\n",
    "## Deep CNN\n",
    "\n",
    "* for the parameters of convolution filters, formulas above apply\n",
    "* common architecture -> convolution filter shrink and number of channels increases\n",
    "* in the last step, filters are unrolled to one long vector which is fed into output layer\n",
    "\n",
    "**Pooling layer**  \n",
    "* max pooling -> input matrix divided into regions, returns max for each region\n",
    "    * filter size & stride as pooling hyper params (defines regions)\n",
    "    * no parameters to learn (fixed function)\n",
    "    * intuition -> in case filter feature is detected, keep large number (the feature was detected)\n",
    "    * done independently on each of the channel\n",
    "    * ([(n+2p-f)/s+1] x [(n+2p-f)/s+1] x number of channels)\n",
    "    \n",
    "* average pooling -> input matrix divided into regions, returns avg for each region\n",
    "    * not used that often (unless in very deep NNs for reducing the input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "* written digit detection problem \n",
    "* input layer\n",
    "    * rgb image\n",
    "    * (32 x 32 x 3)\n",
    "* first layer\n",
    "    * first step\n",
    "        * CNN1\n",
    "        * filter (5 x 5) , stride 1, no padding\n",
    "        * 6 filters\n",
    "        * (28 x 28 x 6)\n",
    "    * second step\n",
    "        * Pool1\n",
    "        * max-pooling operation\n",
    "        * filter (2 x 2), stride 2, no padding\n",
    "        * (14 x 14 x 6)\n",
    "    * first layer -> by convention based on learnable params\n",
    "\n",
    "* second layer\n",
    "    * first step\n",
    "        * CNN2\n",
    "        * filter (5 x 5), stride 1, no padding\n",
    "        * (10 x 10 x 16)\n",
    "    * second step\n",
    "        * Pool2\n",
    "        * max-pooling operation\n",
    "        * filter (2 x 2), stride 2\n",
    "        * (5 x 5 x 16)\n",
    "        * flatten to 400 units\n",
    "\n",
    "* third layer \n",
    "    * FC3 (fully connected)\n",
    "    * 120 relu units\n",
    "    * $W^{[3]} (120, 400)$\n",
    "\n",
    "* forth layer\n",
    "    * FC4 (fully connected)\n",
    "    * 84 relu units\n",
    "\n",
    "* output layer\n",
    "    * 10 softmax units\n",
    "\n",
    "| | Activation shape | Activation size (gradually decreases) | # parameters |\n",
    "| - | - | - | - |\n",
    "| Input | (32, 32, 3) | 3,072 | 0 |\n",
    "| CNN1 (f=5, s=1) | (28, 28, 6) | 4,704 | 456 |\n",
    "| Pool1 | (14, 14, 6) | 1,176| 0 |\n",
    "| CNN2 (f=5, s=1)| (10, 10, 16) | 1,600 | 2,416 |\n",
    "| Pool2 | (5, 5, 16) | 400| 0 |\n",
    "| FC3 | (120, 1) | 120 | 48,120 (connection tu every unit from previous layer + bias -> 120*(400+1))|\n",
    "| FC4 | (84, 1) | 84 | 10,164 (connection to every unit from previous layer + bias -> 84*(120+1))|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Why convolutions\n",
    "\n",
    "* parameter sharing and sparse connections\n",
    "* imagine network from the previous example\n",
    "    * input layer\n",
    "        * rgb image\n",
    "        * (32 x 32 x 3)\n",
    "        * 3,072 activations\n",
    "    * first layer\n",
    "        * first step\n",
    "            * CNN1\n",
    "            * filter (5 x 5) , stride 1, no padding\n",
    "            * 6 filters\n",
    "            * (28 x 28 x 6)\n",
    "            * 4,704 activations\n",
    "        * second step\n",
    "            * Pool1\n",
    "            * max-pooling operation\n",
    "            * filter (2 x 2), stride 2, no padding\n",
    "            * (14 x 14 x 6)\n",
    "    * for fully connected network, this would mean $3,072\\ *\\ 4,704 \\sim 1.4 * 10^7$ parameters\n",
    "    * for convolution network, this means $(5\\ *\\ 5\\ *3 + 1) * 6$ params per filter, thus 456 params in total, this is based on equation $(f^{[l]}\\cdot f^{[l]}\\cdot n_c^{[l-1]}+1) \\cdot n_c^{[l]}$\n",
    "    \n",
    "    Parameter sharing  \n",
    "    * filters are reused across whole image (valid for both low and high-level features), in other words filter parameters are shared\n",
    "    \n",
    "    Sparsity of connections  \n",
    "    * in each layer, output depends only on small number of inputs (sliding window region), in other words connections between layers are \n",
    "    \n",
    "**Backward steps**\n",
    "* loss & cost analogous to traditional MLP\n",
    "* unfortunately no details were shared"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
