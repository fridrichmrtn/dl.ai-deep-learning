{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "* RNN->GRU->LSTM (increasing complexity)\n",
    "    * sequential (dependent on previous tokens)\n",
    "    * can we run the computation in parallel?\n",
    "* [paper](https://arxiv.org/pdf/1706.03762)\n",
    "* Attention + CNN\n",
    "    * self-attention (computing all attention representation at once)\n",
    "    * multi-head attention (multiple versions of attention representations)\n",
    "\n",
    "## Self-attention\n",
    "\n",
    "* A(q,K,V) = attention-based vector representation of a word\n",
    "    * calculate A for each word, ie $A^{<1>},...,A^{<T_x>}$\n",
    "    * representation depends on the context of A (other As in the sentence)\n",
    "    * transformer attention $A(q,K,V) = \\sum_i \\frac{exp(q \\cdot k^{<i>})}{\\sum_j exp(q \\cdot k^{<i>})} v^{<i>} $\n",
    "        * for every word we have query (q), key (k) and value (v)\n",
    "        * ie we have a third word in a sentence $x^{<3>}$\n",
    "            * $q^{<3>} = W^Q \\cdot x^{<3>}$,\n",
    "            * $k^{<3>} = W^K \\cdot x^{<3>}$,\n",
    "            * $v^{<3>} = W^V \\cdot x^{<3>}$,\n",
    "            * $W$ are learned parameters\n",
    "        * intuition\n",
    "            * $q^{<3>}$ is a question about $x^{<3>}$\n",
    "            * we compute $q^{<3>} \\cdot k^{<1>}$ to understand how good is the first word answer to the question, we compute this for every word in a sentence\n",
    "            * transformer attention over the results to get $A^{<3>}$\n",
    "        * in literature $Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}V)$\n",
    "\n",
    "## Multi-head attention\n",
    "\n",
    "* self-attention to a sequence is called a \"head\", repeating that multiple times \"multi-head\"\n",
    "* MultiHead(Q,K,V)\n",
    "    * first head\n",
    "        * matrices $W^Q_1, W^K_1, W^V_1$ are learned and address the relationship\n",
    "        * $Attention(W^Q_1Q, W^K_1K, W^V_1K)$\n",
    "        * mechanism asks and answers a first questions (ie what is happening?)\n",
    "    * second head\n",
    "        * matrices $W^Q_2, W^K_2, W^V_2$ are learned and address the relationship\n",
    "        * $Attention(W^Q_2Q, W^K_2K, W^V_2K)$\n",
    "        * mechanism asks and answers a second questions (ie when happening?)\n",
    "    * ...\n",
    "    * attention heads are concatenated and multiplied by learned weights\n",
    "    * all heads can be computed in parallel\n",
    "* TLDR\n",
    "    * $head_i = Attention(W^Q_iQ, W^K_iK, W^V_iK)$\n",
    "    * $MultiHead(Q,K,V) = concat(head_1, head_2, ..., head_h)W_0$\n",
    "\n",
    "## Transformer network\n",
    "    \n",
    "* encoder (repeated n-times)\n",
    "    * inputs sequence and its corresponding embedding, incl \\<SOS\\>, \\<EOS\\> tokens\n",
    "    * multi-head attention layer\n",
    "    * feed-forward neural network\n",
    "* decoder (repeated n-times)\n",
    "    * getting start of the sequence (or the part of translation done so far)\n",
    "    * multi-head attention layer, inputs start of the sequence calculating Q, K, V\n",
    "    * multi-head attention layer, inputs Q from previous layer and K,V from encoder\n",
    "    * feed-forward neural network\n",
    "    * linear & softmax layers\n",
    "    * next token in sequence\n",
    "* positional encoding is part of input to both encoder and decoder\n",
    "    * represents position of input, through sin & cos functions\n",
    "    * same dimensions as a words embedding vector\n",
    "    * read values off multiple sin/cos funcs to reflect the positions\n",
    "    * added directly to the word embedding vector\n",
    "    * also passed to the net through residual connections (input to enc end dec)\n",
    "* add & norm layers to speed-up convergence after multi-heads and ffs\n",
    "* masked multi-head attention\n",
    "    * used for training for masking \"the future\" part of sequence\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
