{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization algorithms\n",
    "\n",
    "## Mini-batch gradient descent\n",
    "\n",
    "* optimization steps on subset of training examples (mini-batches)\n",
    "* mini-batch $t$ is described with $X^{\\{1\\}}: (n_x, batch\\ size), Y^{\\{1\\}}: (1, batch\\ size)$\n",
    "* algorithm\n",
    "    * for each batch (loop)  \n",
    "        (vectorized)  \n",
    "        * forward propagation\n",
    "        * costs (scaled)\n",
    "        * back prop\n",
    "        * update  \n",
    "        \n",
    "        (/vectorized)  \n",
    "        \n",
    "    (/loop)\n",
    "* epoch - single pass through the training set\n",
    "* mini-batch size is picked in between $n_x$ and 1, balancing computational costs and convergence (noise and speed)\n",
    "* small training set (n<2000) use batch, typical mini-batch sizing 2^6, ... , 2^10\n",
    "\n",
    "## Exponentially weighted averages\n",
    "\n",
    "* smoothing function $\\theta$, $V_t = \\beta V_{t-1} + (1-\\beta) \\theta_t$\n",
    "    * averaging over $\\frac{1}{1-\\beta}$ observations, this comes from the exponent here $(1-\\epsilon)^{1/\\epsilon}=\\frac{1}{e}$\n",
    "    * high $\\beta$ means very smooth lagged result\n",
    "    * low $\\beta$ means more rough immediate result\n",
    "* bias correction (imputing missing values at the begining of the calculation)\n",
    "    * $V_t' = \\frac{V_t}{1-\\beta^t}$\n",
    "\n",
    "## Gradient descent with momentum\n",
    "\n",
    "* algorithm\n",
    "    * during iteration t:\n",
    "        * compute $dW$, $db$ on current batch\n",
    "        * compute $V_{dW} = \\beta V_{dW} + (1-\\beta) dW$\n",
    "        * compute $V_{db} = \\beta V_{db} + (1-\\beta) db$\n",
    "        * can be interpreted as current velocity + acceleration, beta introducing friction, that is why the method is called momentum\n",
    "        * update weights as usual $W = W - \\alpha V_{dW}$, $b = b - \\alpha V_{db}$\n",
    "* $\\beta = 0.9$ is common value of the hyperparam\n",
    "* usually works better than gradient descent\n",
    "\n",
    "## RMSprop\n",
    "\n",
    "* algorithm\n",
    "    * during interation t:\n",
    "        * compute $dW$, $db$ on current batch\n",
    "        * compute $S_{dW} = \\beta S_{dW} + (1-\\beta) dW^2$\n",
    "        * compute $S_{db} = \\beta S_{db} + (1-\\beta) db^2$        \n",
    "        * update weights using scaling factors $W = W - \\alpha \\frac{dW}{\\sqrt{S_{dW}}}$, $b = b - \\alpha \\frac{db}{\\sqrt{S_{db}}}$\n",
    "* different way of scaling changes in fast/slow directions towards global extreme\n",
    "* denominator may lead to explosion of the updates (consider adding small constant)\n",
    "\n",
    "## Adam\n",
    "\n",
    "* adaptive moment estimation, based momentum and rmsprop\n",
    "* algorithm\n",
    "    * $V_{dW} = 0$, $S_{dW} = 0$, $V_{db} = 0$, $S_{db} = 0$\n",
    "    * during iteration t:\n",
    "        * compute $dW$, $db$ on current batch\n",
    "        * Momentum\n",
    "            * compute $V_{dW} = \\beta_1 V_{dW} + (1-\\beta_1) dW$\n",
    "            * compute $V_{db} = \\beta_1 V_{db} + (1-\\beta_1) db$\n",
    "        * Adam\n",
    "            * compute $S_{dW} = \\beta_2 S_{dW} + (1-\\beta_2) dW^2$\n",
    "            * compute $S_{db} = \\beta_2 S_{db} + (1-\\beta_2) db^2$\n",
    "        * Bias correction\n",
    "            * $V_{dW}^{corrected} = V_{dW}/(1-\\beta_1^t)$, $V_{db}^{corrected} = V_{db}/(1-\\beta_1^t)$\n",
    "            * $S_{dW}^{corrected} = S_{dW}/(1-\\beta_2^t)$, $S_{db}^{corrected} = S_{db}/(1-\\beta_2^t)$ \n",
    "        * weight update\n",
    "            * $W = W-\\alpha \\frac{V_{dW}^{corrected} }{\\sqrt{S_{dW}^{corrected}}+\\epsilon}$\n",
    "            * $b = b-\\alpha \\frac{V_{db}^{corrected} }{\\sqrt{S_{db}^{corrected}}+\\epsilon}$\n",
    "* hyperparams\n",
    "    * $\\alpha$ -> needs to be tuned\n",
    "    * $\\beta_1$ -> 0.900 (first moment)\n",
    "    * $\\beta_2$ -> 0.999 (second moment)\n",
    "    * $\\epsilon$ -> 10^-8\n",
    "\n",
    "## Learning rate decay\n",
    "\n",
    "* reducing learning rate for further iterations (big steps at the begining, slowing the steps)\n",
    "* 1 epoch - 1 pass through data\n",
    "* $\\alpha = \\frac{1}{1+decay\\ rate \\cdot epoch\\ num} \\alpha_0$\n",
    "* other methods (exponential decay, discrete, manual, ...)\n",
    "* hyperparams\n",
    "    * $\\alpha_0$\n",
    "    * $decay\\ rate$\n",
    "\n",
    "## Local optima\n",
    "\n",
    "* high-dimensional spaces, zero gradients are commonly saddle points, convex vs concave shapes\n",
    "* plateaus are the problem (!) - long slow path on the surface\n",
    "* advanced optimization algorithms speed up the learning for the plateaus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
